---
title: "BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese"
category: articles
permalink: "/articles/2021-09-20-bartpho/"
date: 2021-09-20
link: https://arxiv.org/pdf/2109.09701
---

[comment]: <> (<a href="https://arxiv.org/pdf/2109.09701">Download PDF here</a>.)
Nguyen Luong Tran, <b> Duong Minh Le </b> and Dat Quoc Nguyen

Abstract: We present BARTpho with two versions—BARTphoword and BARTphosyllable—the first public large-scale monolingual sequence-to-sequence models pre-trained for Vietnamese. Our BARTpho uses the “large” architecture and pre-training scheme of the sequence-to-sequence denoising model BART, thus especially suitable for generative NLP tasks. Experiments on a downstream task of Vietnamese text summarization show that in both automatic and human evaluations, our BARTpho outperforms the strong baseline mBART and improves the state-of-the-art. We release BARTpho to facilitate future research and applications of generative Vietnamese NLP tasks. Our BARTpho models are available at: <a href="https://github.com/VinAIResearch/BARTpho">this https URL</a>.